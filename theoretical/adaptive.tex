%%%%%%%%%%%%%%%%%%%
% SECTION: Adaptive
%%%%%%%%%%%%%%%%%%%
\subsection{Adaptive Learning Methods}

The objective of training an ANN is to perform optimization by minimizing the error using backpropagation. \textbf{Gradient Descent} is one of the most popular algorithms for this task, it is also the most common algorithm for optimizing a neural network.

The task is to follow the slope of the surface until a minimum (local minimum) is reached. Gradient descent is an algorithms that was first used to minimize an objective function $J(\theta)$. This objective function is parametrized by the parameters $\theta\in\mathbb{R}^d$ of the model. SGD updates the parameters in the opposite direction of the gradient from the objective function $\nabla\theta J(\theta)$ w.r.t. to the parameters. This gradient is then used to traverse the surface of the objective function. The \textbf{learning rate $\eta$} determines the size of the steps taken to reach this minimum. \cite{Ruder2016AnAlgorithms}

In recent years, there as been a development of new learning methods aside from SGD. These algorithms have tried to solve problems and challenges that SGD has. The following algorithms are widely used in deep learning.

\subsubsection{Momentum}
\textbf{Momentum} is a method that helps accelerate SGD in the significant direction and get faster convergence and also diminish the oscillations. To obtain this behaviour, momentum adds a fraction $\gamma$ of the update vector of the past time step to the current update vector as shown in equation \ref{eq:momentum}. \cite{Sutton1986TwoNetworks}

\begin{equation} \begin{split} \label{eq:momentum}
    v_{t} = \gamma v_{t-1} + \eta\nabla{\theta}J(\theta) \\
	\theta= \theta - v_{t}
\end{split} \end{equation}

To obtain these two benefits, the momentum term $\gamma$ increases when the gradients of the dimensions point in the same direction and reduces updates when the gradients of the dimensions change directions. \cite{Ruder2016AnAlgorithms}

\subsubsection{Adagrad}
\textbf{Adagrad (Adaptive Gradient)} is an algorithm that adapts the learning rate to the parameters. The algorithm makes a larger update for parameters that are not frequent and smaller updates when the parameters are frequent. The main benefit of Adagrad is that it is no longer needed to manually tune the learning rate. Adagrad uses different learning rate for every parameter $\theta _{i}$ at each time step $t$. The main weakness is that it accumulates the squared gradient in the denominator and the sum keeps growing. This leads to the learning rate to shrink and become extremely small where it can no longer acquire knowledge from the gradient. \cite{Hu2011AdaptiveOptimization} 

The gradient of the objective function, equation \ref{eq:adagrad-gradient} \cite{Hu2011AdaptiveOptimization}, is defined as $g_{t,i}$ w.r.t. to the parameter $\theta_{i}$ at time step $t$.

\begin{equation} \label{eq:adagrad-gradient}
	g_{t,i} = \Delta _{\theta}J(\theta_{t,i})
\end{equation}

In its update rule, equeation \ref{eq:adagrad-update} \cite{Hu2011AdaptiveOptimization}, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_{i}$ based on the past gradients that have been computed for $\theta_{i}$.

\begin{equation} \label{eq:adagrad-update}
	\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_{t}+\varepsilon }} \odot g_{t}
\end{equation}

\subsubsection{Adadelta}
\textbf{Adadelta} tries to solve the problem of the shrinking rate of Adagrad. To solve it, Adadelta restricts the accumulation of past gradient with a windows size $w$. \cite{Zeiler2012ADADELTA:Method}

\subsubsection{RMSprop}
\textbf{RMSprop} is a method introduced by Hinton \cite{HintonNeuralDescent}, that also tries to solve the issue with the shrinking learning rate of Adagrad. Similar to what Adadelta does, RMSprop divides the learning rate by an exponentially decaying average of squared gradients \cite{Ruder2016AnAlgorithms}.

FORMULA

\subsubsection{Adam}
\textbf{Adam (Adaptive Moment Estimation)} calculates adaptive learning rates, that are calculated for all parameters. Like Adadelta and RMSprop, Adam also calculates the exponentially decaying average of past squared gradients. Adam, however, also keeps an exponentially decaying average of past gradients $m_{t}$ like Momentum. \cite{Kingma2015Adam:Optimization}

FORMULA


